# ray-cluster.runtime-install.yaml
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: raycluster-kuberay-runtime-install
spec:
  rayVersion: '2.44.1'
  headGroupSpec:
    rayStartParams:
      num-cpus: "0"
    template:
      spec:
        nodeSelector:
          cloud.google.com/gke-nodepool: default
        volumes:
        - name: shared-pip-packages
          emptyDir: {}
        initContainers:
        - name: pip-installer
          image: anyscale/ray-llm:2.44.1-py311-cu124
          command:
            - "sh"
            - "-c"
            - |
              # Install packages into the shared volume.
              pip install --no-cache-dir --target=/pip-packages \
                "xgrammar==0.1.11" \
                "pynvml==12.0.0" \
                "hf_transfer==0.1.9" \
                "tensorboard==2.19.0" \
                "git+https://github.com/hiyouga/LLaMA-Factory.git@ac8c6fdd3ab7fb6372f231f238e6b8ba6a17eb16#egg=llamafactory"
          volumeMounts:
          - name: shared-pip-packages
            mountPath: /pip-packages
        containers:
        - name: ray-head
          image: anyscale/ray-llm:2.44.1-py311-cu124
          resources:
            limits:
              cpu: "4"
              memory: "16G"
            requests:
              cpu: "4"
              memory: "16G"
          ports:
          - containerPort: 6379
            name: gcs-server
          - containerPort: 8265 # Ray dashboard
            name: dashboard
          - containerPort: 10001
            name: client
          env:
            - name: PYTHONPATH
              value: "/pip-packages"
            - name: HF_HUB_ENABLE_HF_TRANSFER
              value: "1"
          volumeMounts:
          - name: shared-pip-packages
            mountPath: /pip-packages
            readOnly: true
  workerGroupSpecs:
  - replicas: 4 # NOTE: 4 workers, 1 per GPU
    minReplicas: 1
    maxReplicas: 4
    groupName: workergroup
    rayStartParams: {}
    template:
      spec:
        nodeSelector:
          cloud.google.com/gke-nodepool: l4singlenodepool
        tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
        volumes:
        - name: shared-pip-packages
          emptyDir: {}
        initContainers:
        - name: pip-installer
          image: anyscale/ray-llm:2.44.1-py311-cu124
          command:
            - "sh"
            - "-c"
            - |
              pip install --no-cache-dir --target=/pip-packages \
                "xgrammar==0.1.11" \
                "pynvml==12.0.0" \
                "hf_transfer==0.1.9" \
                "tensorboard==2.19.0" \
                "git+https://github.com/hiyouga/LLaMA-Factory.git@ac8c6fdd3ab7fb6372f231f238e6b8ba6a17eb16#egg=llamafactory"
          volumeMounts:
          - name: shared-pip-packages
            mountPath: /pip-packages
        containers:
        - name: ray-worker
          image: anyscale/ray-llm:2.44.1-py311-cu124
          resources:
            limits:
              cpu: "10"
              memory: "40Gi"
              nvidia.com/gpu: "1"
            requests:
              cpu: "10"
              memory: "40Gi"
              nvidia.com/gpu: "1"
          env:
            - name: PYTHONPATH
              value: "/pip-packages"
            - name: HF_HUB_ENABLE_HF_TRANSFER
              value: "1"
          volumeMounts:
          - name: shared-pip-packages
            mountPath: /pip-packages
            readOnly: true